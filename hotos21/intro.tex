%-------------------------------------------------------------------------------
\section{Introduction}
%-------------------------------------------------------------------------------
Web application developers today have more incentives than ever to provide better privacy for their
users.
%
Laws like the EU's General Data Protection Regulation (GDPR)~\cite{eu:gdpr} and California's
Consumer Privacy Act (CCPA)~\cite{ca:privacy-act} codify users' right to be forgotten, and restrict
any data retention to to anonymized information.
%
Legal consequences and the reputational damage associated with data breaches~\cite{breach:amazon,
breach:twitter, breach:fb, breach:marriott, breach:quora} make it good practice to minimize the user
data retained at any point.
%

%
Although many developers are well-intentioned, getting privacy transformations---such as user
unsubscription---right is challenging.
%
They must perform data privacy transformations to adhere to the privacy policy, while ensuring that
the application retains data required for legal purposes or to preserve application utility, and
without violating application invariants.
%
For example, unsubscription of one user should not allow another user to view content that she
originally could not access.

%\lyt{Frans: this paragraph is too abstract, should lead with examples (and then maybe we don't even
%need this?). Also should not apologize for not guaranteeing privacy, but instead enforce that it's
%the developer's job to write the right spec in S3/4.}
%Transformations that achieve better user privacy requires careful handling of data deletion,
%anonymization, and structural decorrelation, where developers must consider how to handle both
%clearly identifying content (\eg usernames), and also structural correlations between data records
%that can identify the user.
%While these transformations do not enable formal privacy \emph{guarantees}, developers can improve
%the state of privacy in applications today by reasoning about often subtle identifying
%correlations.

Privacy transformations must handle both identifying data content, as well as subtly identifying
correlations between data entities. For example, anonymized public running routes correlated with
the same location can identify the user's hometown and be reassociated with the user;
%anonymized posts on Reddit correlated with a subreddit with very few subscribers can be associated
%back to a single user;
papers' affiliation and reviewer conflicts in HotCRP can reidentify the author; and anonymized order
history can reidentify the buyer.

As privacy policies become more complex, the burden of implementing the corresponding privacy
transformation correctly grows as well.  Consequently, today's applications often support only
coarse-grained and simple privacy policies, implemented using ad-hoc methods.\lyt{perhaps ad-hoc
isn't the right word; maybe "coarse-grained" already summarizes what we want to say here? I think
perhaps what we want to convey is that developers don't follow any standard, agreed-upon way to
implement policies, and rely on whatever hastily cobbled-together solutions work for these
simple policies.}

%We next describe a wide range of privacy transformations, some from existing applications' privacy policies,
%and others that demonstrate the potential for better, more nuanced privacy policies. Implementing
%these transformations using ad-hoc methods places undue labor on the developer and, as policies grow
%more complex, becomes more error-prone.

To systematically address these challenges, we propose \emph{data disguising}, a new framework for
specifying and implementing transformations for privacy policies.
%
With data disguising, developers specify transformations required in privacy policies as high-level \emph{data
disguises}. Applying a disguise transforms the state of application data to, \eg, hide a users'
identifiers.
%
%Disguises consist of transformations performed on the high-level object graph embedded in
%database-based applications (encoded by \eg foreign key relationships in relational
%databases)~\cite{orms}.
%Data masks the state of the object graph embedded in database-based applications (\eg
%encoded by foreign key relationships) after mask application.
%and constrains the state of the object graph embedded in
%database-backed applications (\eg encoded by foreign key relationships).
%
A data disguiser system takes a disguise and its target, and automatically translates the
disguise into the appropriate database transformations to achieve the disguised state. With
disguises, developers need to reason only about constraints on the entity graph, and can avoid the
labor of manually implementing the disguise.

\section{The Need for Privacy Transformations}
\label{sec:survey}

%
We surveyed several widely-used web applications to understand what privacy-increasing operations
they apply on user unsubscription.
%
A set of common themes emerged.
%~\cite{facebook:privacy, twitter:privacy, hotcrp:privacy, reddit:privacy,
%github:privacy, hackernews:privacy, strava:privacy, linkedin:privacy, stackoverflow:privacy,
%wikipedia:privacy, amazon:privacy, prestashop:privacy, spotify:privacy, lobsters:privacy}:
%
Some services that publicly display user contributions (\eg Wikipedia edit
history~\cite{wikipedia:privacy}, StackOverflow answers~\cite{stackoverflow:privacy},
Strava routes~\cite{strava:privacy}) keep them publicly and indefinitely available even if a user deletes
their account.
%
Social networking platforms, which fundamentally thrive off users' \emph{shared} data, keep
contributions directly shared with another user unanonymized and visible to the recipient
(\eg Facebook/Twitter private messages~\cite{facebook:privacy, twitter:privacy},
LinkedIn updates~\cite{linkedin:privacy}).
%
Other platforms with mostly public content keep user contributions visible to the intended audience,
but anonymize them by reattributing the contribution to a placeholder user (\eg GitHub's
@ghost~\cite{github:privacy}, Reddit and Lobsters'
``[deleted]''~\cite{reddit:privacy, lobsters:privacy}).
%
%    \item Keep certain user contributions unanonymized and visible to its intended audience (\eg
%        HotCRP, Lobsters, Wikipedia, HackerNews~\cite{hotcrp:privacy, lobsters:privacy,
%        hackernews:privacy, wikipedia:privacy}).
%    \item Delete user contributions on user profile or feed (\eg Facebook,
%        Twitter~\cite{facebook:privacy, twitter:privacy}).
%\end{itemize}
%
All applications surveyed retain some information for legal or necessary business
purposes.
%
For open-source applications, inspection shows that developers implement these transformations
via ad-hoc database operations, and only support explicit, complete user account deletion (a
rare event).
%

%
We argue for a more \emph{systematic} treatment of privacy transformations, making them a
first-class citizen in application design.
%
In partiucal, we imagine developers declaratively specify the above and other transformation
policies for their application's data, similar to a relational schema.
%
This would also allow for new policies and use cases that both end-users and service operators
benefit from.
%
In particular, our \emph{data disguising} approach supports the following policies and concepts,
which are missing from today's applications but easily described via disguises.
%

\paragraph{Nuanced policies.}
%
Users---and application developers---can benefit from more nuanced privacy policies.
%
For example, a confidential paper review system like HotCRP must keep a user's contributions
(papers, reviews) to preserve utility for others, but may associate each review with a different
placeholder to avoid accidentally revealing the unsubscribed reviewer's identity.
%
Likewise, contributions with a shared property (\eg posts on Reddit that share a common tag)
might be removed entirely to avoid inference attacks, or retained and decorrelated from the
property (\eg keeping the user's Reddit posts, but removing their tags).
%
Similar policies could apply \emph{only} if the property was created by the user (\eg keeping
the user's Reddit posts, but removing any user-created tags), or if the user's contributions
comprise more than a threshold percentage of the contributions with a shared property.
%(\eg remove the user's posts on Reddit with tag $t$ if these posts comprise more than 10\%
%of all posts with tag $t$).
%
Individual users may even specify different preferences for their data.
%
A privacy transformation framework is necessary to turn these preferences into concrete
operations without undue developer burden.
%

\paragraph{Data decomposition.}
%
Applications could go beyond simple account deletion and support a data expiration policy that
anonymizes a user's contributions after the user has been inactive for a period of time, and
restores the user's profile and contributions if the user ever logs back in.
%
Or the application could gradually ``decompose'' sensitive data by applying a series of
privacy transformations that incrementally remove more identifiable information from it as it
ages.
%

\paragraph{Reversibility.}
%
Many applications might wish to employ \emph{reversible} transformations, going beyond permanent
and irrevokable unsubscription.
%
After all, if services must allow users to remove their data on request, it's in the operator's
interest to make it easy for users to change their mind and return.
%
An advanced reversible transformation might, for example, record all actions performed in an
encrypted log, and offer that log for download or push it to third-party cloud storage.
%
If the user wishes to return, they simply supply the log and the transformation reverses.
%
To ensure access to the log even if the user loses their key, the transformation might
secret-share the encryption key~\cite{secretsharing} among the user, the service, and a trusted
third party (\eg the ACLU, or EFF).
%




