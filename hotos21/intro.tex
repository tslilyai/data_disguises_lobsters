%-------------------------------------------------------------------------------
\section{Introduction}
%-------------------------------------------------------------------------------
Web application developers today have more incentives than ever to provide better privacy for their
users.
%
Laws like the EU's General Data Protection Regulation (GDPR)~\cite{eu:gdpr} and California's
Consumer Privacy Act (CCPA)~\cite{ca:privacy-act} codify users' right to be forgotten, and restrict
any data retention to anonymized information.
%
Legal consequences and the reputational damage associated with data breaches~\cite{breach:amazon,
breach:twitter, breach:fb, breach:marriott, breach:quora} make it good practice to minimize the user
data retained at any point.
%

%
Although many developers are well-intentioned, they must today implement \emph{privacy
transformations} (such as user account deletion) 
manually, which results in simplistic and ad-hoc solutions (\S\ref{sec:survey}).
%
To get privacy transformations right, developers must carefully map the high-level privacy
policy to operations that delete or rewrite data objects, ensuring that the application preserves
utility for other users, retains legally-mandated anonymized data, and avoids violating
application invariants.
%
For example, deleting a user's account should not allow another user to view content they could
not originally access, nor make non-sensitive shared content disappear.
%

%
Privacy transformations must correctly handle identifying data as well as subtly identifying
correlations between data objects.
%
For example, anonymized public running routes correlated with the same location can identify the
user's hometown and be reassociated with the user~\cite{strava:heatmap};
%\ms{cite Strava privacy issues?};
%anonymized posts on Reddit correlated with a subreddit with very few subscribers can be
%associated back to a single user;
papers' affiliation and reviewer conflicts in HotCRP can reidentify the author; and an anonymized
order history of an e-commerce site can reidentify the buyer.
%

%
The burden of implementing privacy transformations grows with the underlying privacy policy's
complexity.
%
But more nuanced privacy policies are important and useful!
%
Such policies can help protect users against data correlation attacks; they can give more
control to individuals by allowing them to choose their own, fine-grained privacy semantics; and
they may enable new privacy modes such as data that gradually becomes less identifyable over
time.
%
Likewise, ``reversible'' privacy transformations might strike a sweet spot: they allow for
policies that let users remove identifiable information temporarily, but accommodate the service
providers' interest to make it easy for those users to return.
%
But the developer burden to implement the these transformations is prohibitive today.
%imposed by implementing the transformations required to enact these policies
%

%We next describe a wide range of privacy transformations, some from existing applications' privacy policies,
%and others that demonstrate the potential for better, more nuanced privacy policies. Implementing
%these transformations using ad-hoc methods places undue labor on the developer and, as policies grow
%more complex, becomes more error-prone.

%
To systematically address these challenges, we propose \emph{data disguising}, a new framework
for specifying and implementing privacy transformations.
%
With data disguising, developers specify transformations required in privacy policies as
high-level \emph{data disguises} over existing application data types and associations.
%
Applying a disguise transforms the state of application data to, \eg delete or hide a users'
identifiers or decorrelate identifying object relationships, while preserving application
invariants and utility.
%
%Disguises consist of transformations performed on the high-level object graph embedded in
%database-based applications (encoded by \eg foreign key relationships in relational
%databases)~\cite{orms}.
%
A data disguising tool takes a disguise and its target, and automatically generates the
appropriate database transformations to achieve the disguised state, alleviating the developer
burden.
%


\section{The Need for Privacy Transformations}
\label{sec:survey}

%
We surveyed several widely-used web applications to understand what privacy transformations
they apply on user account deletion.
%
A set of common themes emerged.
%~\cite{facebook:privacy, twitter:privacy, hotcrp:privacy, reddit:privacy,
%github:privacy, hackernews:privacy, strava:privacy, linkedin:privacy, stackoverflow:privacy,
%wikipedia:privacy, amazon:privacy, prestashop:privacy, spotify:privacy, lobsters:privacy}:
%
Some services that publicly display user contributions (\eg Wikipedia edit
history~\cite{wikipedia:privacy}, StackOverflow answers~\cite{stackoverflow:privacy},
Strava routes~\cite{strava:privacy}) keep them publicly and indefinitely available even if a user deletes
their account.
%
Social networking platforms, which fundamentally thrive off users' \emph{shared} data, keep
contributions directly shared with another user unanonymized and visible to the recipient
(\eg Facebook/Twitter private messages~\cite{facebook:privacy, twitter:privacy},
LinkedIn updates~\cite{linkedin:privacy}).
%
Other platforms with mostly public content keep user contributions visible to the intended
audience, but anonymize them by reattributing the contribution to a placeholder user
(\eg GitHub's ``@ghost''~\cite{github:privacy}, Reddit and Lobsters'
``[deleted]''~\cite{reddit:privacy, lobsters:privacy}).
%
%    \item Keep certain user contributions unanonymized and visible to its intended audience (\eg
%        HotCRP, Lobsters, Wikipedia, HackerNews~\cite{hotcrp:privacy, lobsters:privacy,
%        hackernews:privacy, wikipedia:privacy}).
%    \item Delete user contributions on user profile or feed (\eg Facebook,
%        Twitter~\cite{facebook:privacy, twitter:privacy}).
%\end{itemize}
%
All applications surveyed retain some information for legal or necessary business purposes (\eg
Spotify fraud detection~\cite{spotify:privacy}, PrestaShop/Amazon orders~\cite{amazon:privacy,
prestashop:privacy}).
%
In the open-source applications surveyed, developers implement privacy transformations
via ad-hoc database operations, and only trigger them on explicit, user-initiated account
deletion (a rare event).
%
Generally, developers appear to pay little attention to identifying correlations
within the remaining data.
%

%
We argue for a more \emph{systematic} treatment of privacy transformations, making them a
first-class citizen in application design.
%
In particular, we imagine that developers declaratively specify the above and other
transformation policies like they specify a storage structure (\eg a relational schema) today.
%
This also allows for new policies and use cases that benefit both end-users and service operators.
%
Our \emph{data disguising} approach supports the following new policies and concepts,
which are missing from today's applications but easily described via disguises.
%

\paragraph{Nuanced policies.}
%
Users and application developers can both benefit from more nuanced privacy policies.
%
For example, a confidential paper review system like HotCRP must keep a user's contributions
(papers, reviews) to preserve utility for others, but may associate each review with a different
placeholder to avoid accidentally revealing the deleted reviewer's identity.
%
Likewise, contributions with a shared property (\eg posts on Reddit that share a common tag)
might be removed entirely to avoid inference attacks, or retained and decorrelated from the
property (\eg keeping the user's Reddit posts, but removing their tags).
%
Similar policies could apply \emph{only if} the property was created by the user (\eg keeping
the user's Reddit posts, but removing any user-created tags), or if the user's contributions
comprise more than a threshold percentage of the contributions with a shared property.
%(\eg remove the user's posts on Reddit with tag $t$ if these posts comprise more than 10\%
%of all posts with tag $t$).
%
Individual users may even specify different preferences for their data.
%
A privacy transformation framework is necessary to turn these preferences into concrete
operations without undue developer burden.
%

\paragraph{Data decomposition.}
%
Applications could go beyond simple account deletion and support a data expiration policy that
anonymizes a user's contributions after the user has been inactive for a period of time, and
restores the user's profile and contributions if the user ever logs back in.
%
Or the application could gradually ``decompose'' sensitive data by applying a series of
privacy transformations that incrementally removes more identifiable information over time.
%from it as it ages.
%

\paragraph{Reversibility.}
%
Many applications might wish to employ \emph{reversible} transformations, going beyond permanent
and irrevokable account deletion.
%
After all, if services must allow users to remove their data on request, it is in the operator's
interest to make it easy for users to change their mind and return.
%
An advanced reversible transformation might, for example, record all actions performed in an
encrypted log, and offer that log for download or push it to third-party cloud storage.
%
If the user wishes to return, they simply supply the log to reverse the transformation.
%
To ensure access to the log even if the user loses their key, the transformation might
secret-share the encryption key~\cite{secretsharing} among the user, the service, and a trusted
third party (\eg the ACLU or EFF).
%
