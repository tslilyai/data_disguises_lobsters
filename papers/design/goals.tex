%-------------------------------------------------------------------------------
\section{Setting and Goals}
%-------------------------------------------------------------------------------
Our design considers typical database-backed applications that link a library implementing our disguising
tool (\sys).
%
\sys provides an API that the application uses to specify disguises, apply them, and (in some
cases), reveal disguised data with proper authorization (Table~\ref{tab:client_api}).
%
\sys executes the database transformations required as part of a disguise, and stores
disguised data in a secure way, subject to the threat model we describe in \S\ref{s:threat}.

\subsection{Use Cases \sys Should Support}

\head{Example: HotCRP.}
We imagine three useful disguises that aid privacy in HotCRP: 
\begin{enumerate}
    \item Per-user GDPR Account Deletion, which
        removes all of a particular user's data such as reviews and papers.
    \item Universal Comment Removal, which removes all comments on papers.
    \item Universal Decorrelation, which decorrelates users from
        their authored reviews and papers. Decorrelation of a paper or review (1) generates an
        anonymous user in the user table, and (2) rewrites the foreign key from the paper or review
        to the users table to point to the newly created anonymous user.
\end{enumerate}

Users who invoke GDPR deletion are given the option to return by revealing their disguise; however,
between disguising and revealing their account, no other entity learns the identity or existence of
the user (ensuring that the disguise complies with the GDPR).

After several years, the admin can decay identifiers in the conference data by decorrelating all
users from their authored reviews and papers.  This helps protect reviewers and paper authors from
being unblinded in the case of a data breach.

Even after universal decorrelation, it would be nice if HotCRP allowed users to apply GDPR account
deletion to delete (even decorrelated) papers and reviews that they wrote: a disguise that applies
to a user's data may, in some circumstances, need to apply to data decorrelated from the user.

Application of universal decorrelation also should not prevent the application of universal comment
removal: the admin may decide after applying universal decorrelation that arbitrary text in paper
comments also identifies their authors (which are now anonymous users), and invoke universal comment
removal. 
%comment removal can still occur to remove data touched by prior disguises. %If revealed, data
%should return to the previously disguised state.  
Because disguises can remove or further modify data of anonymous users, the application developer
can simply write a new disguise to fix mistakes in the initial disguise that left some identifying
information around.

Finally, HotCRP would retain much of its usefulness if users could be permitted to view reviews on
their papers, or edit their reviews, even if these papers and reviews have been decorrelated and
belong to anonymous users.  However, this should be possible \emph{without} revealing to other users
of the system which actual user authored these anonymized papers and reviews.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\head{Usability.} These disguising properties should be achieved with minimal burden on
users. 

When \sys disguises a user's data for universal decorrelation or comment removal, \sys emails a
unique URL (a \emph{capability}) to the user corresponding to that a paper or review only known to
the user. When the user wants to ask for permission to view (or edit, if permitted) the undisguised
paper or review, the user simply has to click the URL link and log into the application. Thus,
application of universal disguises should not require a user to be online in order to gain the
capability to apply further disguises or view their disguised data.

Similarly, when \sys disguises a user's data for GDPR deletion, the invoking user receives a URL
that allows the user to reveal the disguise, restoring their account.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\head{General Use Cases.} 
From the HotCRP example, we generalize a handful of the most beneficial and practical use cases
that \sys should support:

\begin{itemize}
    \item\textbf{1st-Party GDPR-Compliant Disguising and Revealing.}
\sys should support user-invoked disguise(s) to modify, decorrelate and/or delete the user's data and
account that enable applications to meet the requirements of the GDPR's right to be
forgotten.
%
These disguises can optionally be revealable, which enables users to restore the original state of
        disguised data to \eg permanently restore their accounts and/or data ownership.

\item\textbf{3rd-Party Disguising.}
\sys should support disguise applied by principals (such as an admin) that modify, decorrelate, and/or delete
    data of other principals in the system. For example, universal conference anonymization or
    comment removal alters data not associated with the invoking admin.

\item\textbf{Temporary Recorrelation w/out Database Changes.}
    \begin{itemize}
        \item \emph{For Disguising Decorrelated Data}: 
        A disguise targeting a user's data can, if authorized, also operate on data decorrelated
            from the user by prior disguises without modifying the database's persistent links
            between data and owning users. For example, GDPR account deletion removes user $u$'s
            reviews even if \sys has decorrelated these reviews from $u$.

    \item \emph{For Per-User Revealed Views and Permissions}:
            Users should be able to operate with ownership permissions on data previously correlated
            with their account, but decorrelated by a disguise. Users do so by interacting with a
            personalized, temporary database view when they visit their decorrelated data. This
            should require no modifications to the actual database contents.

            For example, a user whose authored papers have been decorrelated can log in and view
            their papers and reviews on their papers.
\end{itemize}

\item\textbf{Disguising Anonymized Users.}
\sys can disguise data of anonymized users. For example, universal comment removal may removes the
        data of anonymized users generated by universal decorrelation.  This allows disguises to be
        developed over time, and remove identifying data potentially mistakenly missed by prior
        disguises.
\end{itemize}

\subsection{\sys Abstractions}
\sys supports these use cases by exposing three abstractions to applications:
\begin{itemize}
    \item Application \emph{principals}, which correspond to users of the application, 
	and which are uniquely identifiable (\eg via a user ID).
        %natural persons using the application, and
        %
	We denote an application principal as $p \in P$, where $P$ is the set of all application principals.
	%
    \item Application \emph{pseudoprincipals}, which correspond to anonymous users not tied to any natural
    person, and which are uniquely identifiable (\eg via a user ID).
        %
	Pseudoprincipals are part of the set of application principals $p \in P$.
        %\lyt{I don't see why they should be a different set?}
	%
    \item A disguise $d \in D$ invoked by some principal $p$, which 
        transforms the application database.
        %specification that consists of one or more disguises, $d \in D$, which each specify how \sys 
	%Each invocation of $d$ results in a unique \emph{disguise} $d$, identified by
        %a global disguise sequence number $i$.
        %\lyt{We can imagine the sequence number being split by disguise specification, but it
        %doesn't seem important here? All it needs to be is uniquely identifiable.}
	%
\end{itemize}
%
%
\sys operates on two categories of data:
\begin{enumerate}
    \item \emph{\textbf{Database Contents}}: The application reads and writes database
        contents, which include visible, undisguised and currently-disguised data.
        \sys modifies the data database contents when it applies and reveals disguises.
    \item \emph{\textbf{Disguise Diffs}}: \sys records the set of disguise diffs---which contain the
        original database contents and the modifications performed to them---and uses diffs to
        compose disguises on top of one another, or to reveal the updates done by a disguise.
        Every disguise diff has an associated principal (namely the one associated with the modified data).
\end{enumerate}

\head{Data Access Control.}
The application's normal permissions logic controls data access to database contents. The
application invokes \sys to access and needs to parse diff contents directly: \sys exposes an API
(\S\ref{s:api}) allowing the client or application to learn specific semantic information contained
in diffs (\eg whether a particular principal used to own a now-decorrelated piece of data) if
provided appropriate authorization. Clients can additionally request via \sys to access and use diff
information to reveal or apply disguises.

\vspace{6pt}\noindent
Access to diffs is controlled by a pair of \emph{capabilities}:
\begin{enumerate}
    \item \emph{\textbf{Data Capability \dcapa{p}}}: Grants read access to disguise diffs 
        associated with principal $p$.
    \item \emph{\textbf{Locating Capability \lcapa{pd}}}: Allows locating the disguise
        diffs associated with principal $p$ produced from applying $d$, but grants no 
        access to the diffs' data.
\end{enumerate}

\noindent \sys needs both parts of the pair of data and locating capabilities \pcapa{p}{d} to access
disguise diffs.  Each capability in the pair alone is insufficient: an attacker who can spoof
\lcapa{pd} will not be able to access disguise diffs without \dcapa{p}, although they can learn
that $d$ disguised some of $p$'s data; and an attacker holding \dcapa{p} cannot locate the disguise
diffs to access without \lcapa{pd}.

%-------------------------------------------------------------------------------
\subsection{Threat Model}
\label{s:threat}
%-------------------------------------------------------------------------------

%
Data disguises protect user information in a web application against external observation
and service compromise.
%
An external observer is a user of the web application (authenticated or unauthenticated) who
observes information exposed through using the application.
%
A service compromise occurs when an attacker compromises the web application and 
gains full access to the server.
%
The attacker therefore can access any data stored, perform any actions the application can
perform, and access any information available to \sys.
%

%
A data disguise guarantees that the disguised data is hidden from any future attackers unless
explicitly revealed by an authorized principal.
%
In particular, an attacker who compromises \sys at time $t$ learns \emph{nothing but}:
\begin{enumerate}[nosep]
  \item the (plaintext) contents of the application database at or after time $t$;
  \item the disguises invoked, and the identity of the principals invoking them, after time $t$; and 
  \item all of a principal's disguise diffs acquired prior to time $t$, should the principal authorize access to
      disguise diffs for purposes of revealing or performing some application action after time $t$.
      %time $t$.
      %results of revealing, after time $t$, disguises applied prior to $t$.
\end{enumerate}
%
We make standard assumptions about the security of cryptographic primitives: attackers cannot
break encryption and keys stored with non-colluding clients are safe.
%

%
\sys operates in an honest-but-curious setting: even if compromised, \sys faithfully executes
its protocols, but exposes all data accessed to the attacker.
%

\subsection{Security Goals}
%
With this threat model, \sys seeks to meet four security goals:
%

%
\vspace{6pt}\noindent\textbf{\emph{(1) Authorized Disguises.}}
%
Only a client properly authenticated as a principal $p$ who is authorized to invoke $d$ can apply
(and later reveal) $d$.
%

\vspace{6pt}\noindent\textbf{\emph{(2) Secure Disguise Diffs.}}
%
Only an authenticated principal who provides both locating capability \lcapa{pd} and data capability
\dcapa{p} can authorize access to the disguise diffs, as well as any information
(\eg ownership permissions) derived from diffs.
%

\vspace{6pt}\noindent\textbf{\emph{(3) Privacy of Disguise History.}}
%
An attacker cannot learn the set of disguises that have disguised a principal's data.
%
An attacker only learns a disguise $d$ exists if an authenticated principal $p$ provides
locating capability \lcapa{pd} to \sys.

\vspace{6pt}\noindent\textbf{\emph{Non-Goals.}}
%
Under \sys's threat model, the following properties are out of scope:
%
\begin{itemize}
    \item Security of disguised data when an attacker has prior snapshots of the system.
    \item Security of metadata such as the number of disguises applied to, or number of
        disguise diffs associated with, \emph{some} principal (where the principal
        ID is unknown).
    \item Security of metadata when an attacker has \lcapa{pd}: the attacker can learn that $d$
        applied to a particular principal $p$, and the number of disguise diffs for that $p$ and $d$.
    \item Privacy guarantees about undisguised data: if identifying data about $p$ is not covered by
        disguise specification $d$, it remains visible to attackers after corresponding instance $d$ applies.
        For example, a disguise that leaves contents of posts unmodified does not hide identifying references
	to users in that content.
    \item Hiding whether data is disguised or not. For example, if a post's author is anonymized as ``anonFox'',
        \sys leaks the fact that a post's author has been disguised.
    %\item Authorized disguises if an attacker authenticates as a principal $p$. \ms{this seems trivial}
    %\item Security of disguise changes and disguise history applied to $p$ if $p$'s private key, \privk{p}, is compromised.
    %\item Security of disguise changes and disguise history of $d$ applied to $p$ if an
        %authorized client provides \lcapa{pd} and \dcapa{p}. \ms{as above}
\end{itemize}

