\section{Introduction}
\label{s:intro}

%% Problem statement
%
Web applications today handle copious amounts of personal information, much of it sensitive.
%
%Users' privacy and their rights to exercise control over their data in these applications
%thus are an increasingly important concern for application developers.
%
Laws like the GDPR force developer to consider users' data privacy and their rights to their
data; and many developers do want to be ``good citizens'' even beyond meeting the legal
requirements, or are at least worried about the reputational damage of a large-scale data leak.
%
But doing a good job at data privacy requires extensive manual labor today, and applications
consequently grant users little fine-grained control over how they retain and use their private
data.
%

%
%Furthermore, even these efforts are nontrivial, and require careful understanding and manipulation
%of the web application's database. Companies with resources such as FB have implemented internal
%systems that handle e.g., user account deletion (cite DELF); however, solutions like these fail to
%generalize and require time and money that smaller-scale companies may lack.
%

%% Our key idea: data disguising
%
This paper describes \emph{data disguising}, a new systematic framework that provides
developers with abstractions that help them implement correct, expressive, and end-user-friendly
privacy functionality.
%
Data disguises transform a user's data in a web application in privacy-preserving ways, but still
allow the application to maintain key functionality even when a user has disguised (\ie anonymized)
their data.
%
Beyond simply transforming data, data disguising enables data decay (gradual anonymization over time);
revealing of disguises (restoration of deleted data by the user, while providing strong privacy
guarantees when deleted); and decorrelation and temporary recorrelation of users with their data for
the purpose of editing anonymized data, or further disguising it. \lyt{<-- Fix this.}
%
Furthermore, data disguising generalizes across web applications, handles the technical challenges
of securing private data, and integrates with applications' database schemas and semantics.
%

\sys, a prototype data disguising library, enables database-backed applications to
easily support secure data disguising and provide better user privacy. \sys's API supports the
abstraction of users and anonymous users as first-class citizens; and provides secure and private
per-user data transformation and storage, subject to the threat model we describe in \S\ref{s:threat}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Motivation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
To motivate data disguising, we consider how HotCRP, a popular application for conference paper and
review management, can improve user privacy.  HotCRP currently retains data from past conferences
indefinitely, and many conferences often enforce single- or double-blind reviews, meaning that no
one but the reviewer should be able to correlate their reviews or paper comments with their
identity. However, HotCRP's stores reviews indefinitely linked to the identity of the reviewer,
leaving reviewers succeptible to unblinding in case of a data breach.

To improve upon HotCRP's provided user privacy, HotCRP implement the follow disguises using \sys:
\begin{enumerate}
    \item Per-user GDPR Account Deletion: removing all of a particular user's data such as reviews and papers.
    \item Universal Decorrelation: decorrelating users from
        their authored reviews and papers by (1) generating an
        anonymous user, and (2) relinking the paper or review
        to the newly created anonymous user.
    \item Universal Comment Removal: removing all comments on papers.
\end{enumerate}
\sys enables HotCRP to support the following features:
\begin{itemize}
    \item\emph{1st-Party GDPR-Compliant Disguising:}
HotCRP supports user-invoked disguise(s) to modify, decorrelate and/or delete the user's data and
account, meeting the requirements of the GDPR's right to be forgotten.

    \item \emph{1st-Party Disguised Data Restoration}: HotCRP users who invoke GDPR deletion are given the
        option to return by revealing their disguise; however, between disguising and revealing
        their account, no other entity learns the identity or existence of the user (ensuring that
        the disguise complies with the GDPR).

    \item\emph{3rd-Party Disguising.}
\sys supports disguises applied by users (such as an admin) that transform data of other users in the system. For example, universal conference anonymization or
    comment removal alters data not associated with the invoking admin.

\item \emph{Data Decay}: After several years, the admin can decay identifiers in the conference data
    by decorrelating all users from their authored reviews and papers.  This helps protect reviewers
        and paper authors from being unblinded in the case of a data breach.

\item \emph{Temporary Recorrelation with Anonymized Data}: 
Even after universal decorrelation, HotCRP allows users to apply GDPR account
deletion to delete even their decorrelated papers and reviews that they wrote.
%

%
Furthermore, HotCRP users can also view reviews on their papers, or edit their reviews, even if
        these papers and reviews have been decorrelated and belong to anonymous users.  HotCRP users
        interact with a personalized, temporary database view when they visit their decorrelated
        data. 

Data disguising enables temporary recorrelation to support these use cases \emph{without} changing
        the database contents and revealing to other users of the system
which actual user authored these anonymized papers and reviews.

\item \emph{Further Disguising of Anonymized Users' Data}: A disguise such as universal comment
        removal can be applied after universal decorrelation, even though it now applies to data of anonymous users.
This allows disguises to be developed over time, and to remove identifying data potentially mistakenly missed by prior
disguises, but which now belongs to anonymous users.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Importantly, these disguising properties place minimal burden on users.
%
When HotCRP disguises a user's data for universal decorrelation or comment removal, HotCRP emails
the user a unique URL that it gets via invoking \sys's API.  When the user
wants to ask for permission to view (or edit, if permitted) the undisguised paper or review, the
user simply has to click the URL link and log into the application. Thus, application of universal
disguises should not require a user to be online in order to gain the capability to apply further
disguises or view their disguised data.

Similarly, when HotCRP disguises a user's data for GDPR deletion, the invoking user receives a URL
that allows the user to reveal the disguise, restoring their account.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{\sys Abstractions}
An application using \sys's API works with the following \sys abstractions:
\begin{itemize}
    \item Application \emph{principals}, which correspond to users of the application, 
	and which are uniquely identifiable (\eg via a user ID).
        %
	We denote an application principal as $p \in P$, where $P$ is the set of all application principals.
	%
    \item Application \emph{pseudoprincipals}, which correspond to anonymous users not tied to any natural
    person, and which are uniquely identifiable (\eg via a user ID).
        %
	Pseudoprincipals are part of the set of application principals $p \in P$.
    A pseudoprincipal may or may not be indistinguishable from a real principal: for example, it 
        may not have an associated password.
	%
    \item Disguises $d \in D$ %invoked by some principal $p$, which 
        that transform the application database.
	%
    \item \emph{Disguise ownership tokens}: metadata linking
        disguise-produced pseudoprincipals to real-user principals.
    %
    \item \emph{Disguise diff tokens}: metadata recording the \emph{diff} between pre-disguise and
        post-disguise application state.
    %
    \item \emph{Token decryption capability}: authorizes the holder to access disguise tokens. 
        The decryption capability for principal $p$ is a private key \privk{p}.
    %
    \item \emph{Token locator}: locates a subset of disguise tokens belonging to a particular
        principal produced by a particular disguise.
\end{itemize}
Table~\ref{tab:notation} demonstrates how we notate abstractions.

%-------------------------------------------------------------------------------
\subsection{Threat Model}
\label{s:threat}
%-------------------------------------------------------------------------------

\ms{note similarity with CryptDB threat model: only active user's data gets compromised if
attacker is around while they interact with the system; dormant users' data is never exposed.}

%
Data disguises protect user information in a web application against external observation
and service compromise.
%
An external observer is a user of the web application (authenticated or unauthenticated) who
observes information exposed through using the application.
%
A service compromise occurs when an attacker compromises the web application and 
gains full access to the server.
%
The attacker therefore can access any data stored, perform any actions the application can
perform, and access any information available to \sys.
%

%
A data disguise guarantees that the disguised data is hidden from any future attackers unless
explicitly revealed by an authorized principal.
%
In particular, an attacker who compromises \sys at time $t$ learns \emph{nothing but}:
\begin{enumerate}[nosep]
  \item the (plaintext) contents of the application database at or after time $t$;
  \item the disguises invoked, and the identity of the principals invoking them, after time $t$; and 
  \item all of a principal's disguise tokens acquired prior to time $t$, should the principal authorize access to
      disguise tokens for purposes of revealing or performing some application action after time $t$.
      %time $t$.
      %results of revealing, after time $t$, disguises applied prior to $t$.
\end{enumerate}
%
We make standard assumptions about the security of cryptographic primitives: attackers cannot
break encryption and keys stored with non-colluding clients are safe.
%

%
\sys operates in an honest-but-curious setting: even if compromised, \sys faithfully executes
its protocols, but exposes all data accessed to the attacker.
%

\subsection{Security Goals}
%
With this threat model, \sys seeks to meet three security goals:
%

%
\vspace{6pt}\noindent\textbf{\emph{(1) Authorized Disguises.}}
%
Only a client properly authenticated as a principal $p$ who is authorized to invoke $d$ can apply
(and later reveal) $d$.
%

\vspace{6pt}\noindent\textbf{\emph{(2) Secure Disguise Tokens.}}
%
Only an authenticated principal who provides decryption capability
\privk{p} can authorize access to the disguise tokens, as well as any information
(\eg ownership permissions) derived from tokens.
%

\vspace{6pt}\noindent\textbf{\emph{(3) Privacy of Disguise History.}}
%
An attacker cannot learn the set of disguises that have disguised a principal's data.
%
An attacker only learns a disguise $d$ applied to $p$'s data if authenticated principal $p$
interacts with the system to reveal or otherwise access disguised data.
%provides locator \lcapa{pd} to \sys.

\vspace{6pt}\noindent\textbf{\emph{Non-Goals.}}
%
Under \sys's threat model, the following properties are out of scope:
%
\begin{itemize}
    \item Security of disguised data when an attacker has prior snapshots of the system.
    \item Security of metadata such as the number of disguises applied to, or number of
        disguise tokens associated with, \emph{some} principal (where the principal
        ID is unknown).
    %\item Security of metadata when an attacker has \lcapa{pd}: the attacker can learn that $d$
        %applied to a particular principal $p$, and the number of disguise tokens for that $p$ and $d$.
    \item Privacy guarantees about undisguised data: if identifying data about $p$ is not covered by
        disguise specification $d$, it remains visible to attackers after corresponding instance $d$ applies.
        For example, a disguise that leaves contents of posts unmodified does not hide identifying references
	to users in that content.
    \item Hiding whether data is disguised or not. For example, if a post's author is anonymized as ``anonFox'',
        \sys leaks the fact that a post's author has been disguised.
\end{itemize}

\begin{table*}[t!]
\centering
\begin{tabular}{ c p{.8\linewidth} }
\textbf{Notation} & \textbf{Description} \\
\hline
    \vspace{6pt}
$p$ & application principals, corresponding to a user ID in the application.\\
    \vspace{6pt}
$d$ & disguise invoked by authorized principals that transforms the application database in
    privacy-preserving ways.\\
    \vspace{6pt}
\pubk{p} & public key of $p$. \\
    \vspace{6pt}
\privk{p} & private key of $p$, which acts as the decryption capability for $p$'s tokens. \\
    \vspace{6pt}
\tdiff{pd} & diff token associated with $p$ produced by disguise $d$. These tokens store data
    necessary to restore disguised content to its original form.\\
    \vspace{6pt}
\town{pd} & ownership token associated with $p$ produced by disguise $d$. These tokens store links
    between $p$ and pseudoprincipals created by decorrelating $p$'s data.\\
    \vspace{6pt}
\lcapa{pd} & locator to efficiently find the tokens associated with a principal $p$
    produced by disguise $d$.\\
    \vspace{6pt}
    \end{tabular}
\caption{Notation used to describe \sys's design.}
\label{tab:notation}
\end{table*}


