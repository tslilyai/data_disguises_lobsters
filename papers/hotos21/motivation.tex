\section{Challenges of Privacy Transformations}
\label{sec:survey}
%
%We investigate the potential of privacy transformations as a first-class
%citizen in application design, and the challenges to support these
%transformations.
%%
%These challenges argue for a systematic approach to privacy transformations.
%%

%
Today, privacy transformations are typically realized in ad-hoc, application-specific ways.
%
For example, many applications support account deletion (\eg to comply with the GDPR),
although the realization differs depending on application semantics: many applications retain
data for legal or necessary business purposes (\eg Spotify fraud detection~\cite{spotify:privacy},
Amazon orders~\cite{amazon:privacy}); some delete public contributions, but keep private
messages unanonymized and visible to their recipients~\cite{facebook:privacy, twitter:privacy};
and yet others keep public contributions visible but anonymize them, reattributing the contribution
to a placeholder user (\eg GitHub's ``@ghost''~\cite{github:privacy}, Reddit/Lobsters'
``[deleted]''~\cite{reddit:privacy, lobsters:privacy}).
%
While these policy choices are application-specific, implementing the transformation manually
against the database is a burden on application developers, who must bear several requirements
in mind.
%

%
Modifying or deleting data must not compromise application correctness: for example, privacy
transformations must maintain referential integrity of an application's relational database.
%
Developers must be careful to properly transform user data without violating application invariants,
which they do today via ad-hoc database operations in their application code.
%
Furthermore, one application may apply several privacy transformations, intermixed with reversals.
%
When these transformations share dependencies or contradict each other, correctly composing
transformations to achieve the desired privacy properties is a daunting task.
%
%Finally, practical privacy transformations should require minimal application changes.
%

%
To make these challenges concrete, consider implementing two privacy
transformations in HotCRP~\cite{hotcrp}:
%
(1) \texttt{GDPR} implements GDPR's ``right-to-be-forgotten'' removal of user
data; and
%
(2) \texttt{ConfAnon} anonymizes reviewers in the database some time after the
reviewing concludes, preventing reviewer identification in case of a data breach.
%
\texttt{ConfAnon} needs to decorrelate reviews and review metadata (\eg review preferences)
from reviewers by ensuring that nothing associated with a review references a real user's
account by foreign key.
%
To maintain referential integrity, the developer must take care to generate fake user profiles
and update foreign key attributes to reference them.
%
The \texttt{GDPR} transformation, by constrast, must identify all data related to a user and
remove it, though taking care to leave behind shared data such as review text.
%

%
In isolation, each transformation may seem simple; however, composing them is tricky.
%
\texttt{ConfAnon} destroys information that links a user's data to their account by changing
that data to reference anonymous, fake users (\eg rewriting the foreign key for a review's
author to point to a fake user profile).
%
If this reviewer user later invokes \texttt{GDPR}, the transformation needs to know which
data to remove, which is impossible without the original foreign keys.
%
%Due to data dependencies between these transformations, \texttt{GDPR} fails to achieve its desired
%privacy properties, namely the complete removal of the user's data.
%
When applied the other way around (\texttt{GDPR}, then \texttt{ConfAnon}), the transformations
compose more easily, as there is no need to anonymize deleted data.
%
But it falls to the application developer to write privacy transformation code that handles
both sequences of disguises correctly.
%

%\subsection{Utility of Privacy Transformations}
%If we can address these challenges, privacy transformations have the potential to provide many
%privacy benefits to web applications and their users.
%Widely-used web applications today~\cite{spotify:privacy, amazon:privacy,
%strava:privacy, hotcrp:privacy, wikipedia:privacy, facebook:privacy, twitter:privacy,
%reddit:privacy, github:privacy, lobsters:privacy} mainly support variants of an account
%deletion privacy transformation, as requried by, \eg the GDPR~\cite[Art.\ 17]{eu:gdpr}.
%
These challenges only become more urgent if we consider other desirable privacy transformations.
%
%But many applications would benefit from support for other privacy transformations.
%
%Users and application developers can both benefit from \textbf{more nuanced} privacy policies:
%for example, a confidential paper review system like HotCRP~\cite{hotcrp} keeps users'
%contributions (papers, reviews) after they delete their account to preserve utility for others, but
%could additionally associate each review with a different placeholder to avoid revealing the deleted
%reviewer's identity.
%
%Likewise, contributions with a shared property (\eg Reddit posts with a common tag)
%might be removed entirely to avoid inference attacks, or retained and decorrelated from the
%property (\ie keeping the user's Reddit posts, but removing their tags).
%\eddie{Would like more explanation of that}
%In some cases, a policy could preserve utility while reducing the efficacy of later inference
%attacks by \eg modifying sensitive metadata (\eg tags, creation times).
%
%Some transformations should be performed only on sensitive metadata:
%Similar policies could apply \emph{only if} the property was created by the user (\eg keeping
%the user's Reddit posts, but removing any user-created tags),
% tag like “\#cat” is likely insensitive and useful to preserve, whereas one naming a person is not.
%
%(\eg remove the user's posts on Reddit with tag $t$ if these posts comprise more than 10\%
%of all posts with tag $t$).
%
%Individual users could even specify different preferences for their data.
%
%% A privacy transformation framework is necessary to turn these preferences into concrete
%% operations without undue developer burden.
%
%
Applications could go beyond simple account deletion and support \textbf{data
expiration} policies, which anonymize a user's contributions after the user has been inactive for a
period of time, possibly restoring the user's profile and contributions if the user ever logs back
in.
%
Or the application could support gradual \textbf{data decay} policies that ``decay'' sensitive data
by applying incremental privacy transformations over time.

Furthermore, many applications might wish to employ \textbf{reversible} transformations to, for
example, support account reactivation instead of permanent and irrevocable account deletion.
%
After all, if services must allow users to remove their data on request, it is in the operator's
interest as well as the user's to make it easy for a user to change their mind and return, bringing
their data along.

We envision that a systematic approach to privacy transformations can help address the challenges
identified; if we can overcome these challenges, privacy transformations have the potential to enhance application
privacy beyond what we have discussed here.
