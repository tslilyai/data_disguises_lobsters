\section{Privacy Transformations}
\label{sec:survey}

We motivate a systematic treatment of privacy transformations, in which they are a first-class citizen in
application design, by investigating how privacy transformations are used in applications today,
their potential to achieve better privacy in the future, and the challenges we face to support these
transformations.

\subsection{Privacy Transformations In Practice Today}
%
%We surveyed widely-used web applications to understand the privacy
%transformations they apply.
%
Widely-used web applications mainly support an account deletion privacy transformation,
one that \eg the GDPR~\cite[Art.\ 17]{eu:gdpr} mandates.
%
On account removal, all applications surveyed delete some user profile
information (\eg passwords) from underlying databases, but 
retain some information for legal or necessary business purposes
(\eg Spotify fraud detection~\cite{spotify:privacy}, PrestaShop/Amazon
orders~\cite{amazon:privacy, prestashop:privacy}).
%
%The treatment of other user contributions varies.

Some services keep most deleted users' contributions publicly and indefinitely available (\eg
StackOverflow answers~\cite{stackoverflow:privacy}), sometimes associated with the original user
name (\eg Wikipedia edit history~\cite{wikipedia:privacy}).
%shared Strava routes~\cite{strava:privacy}), 
%
Some platforms delete posts, but keep private messages unanonymized and visible to
their recipients (Facebook/Twitter~\cite{facebook:privacy, twitter:privacy});
%LinkedIn updates~\cite{linkedin:privacy}).
%
others keep posts visible but anonymized, reattributing the contribution to a placeholder user
(\eg GitHub's ``@ghost''~\cite{github:privacy}, Reddit/Lobsters'
``[deleted]''~\cite{reddit:privacy, lobsters:privacy}).
%
%    \item Keep certain user contributions unanonymized and visible to its intended audience (\eg
%        HotCRP, Lobsters, Wikipedia, HackerNews~\cite{hotcrp:privacy, lobsters:privacy,
%        hackernews:privacy, wikipedia:privacy}).
%    \item Delete user contributions on user profile or feed (\eg Facebook,
%        Twitter~\cite{facebook:privacy, twitter:privacy}).
%\end{itemize}
%

%In the open-source applications surveyed, developers implement privacy transformations
%via ad-hoc database operations, and only trigger them on explicit, user-initiated account
%deletion.
%
%Most developers appear to pay little attention to identifying correlations
%within the remaining data.
%

\subsection{The Potential of Privacy Transformations}
Privacy transformations can achieve much more than they do today.
%
Users and application developers can both benefit from \textbf{more nuanced} privacy policies:
%
for example, a confidential paper review system like HotCRP~\cite{hotcrp} keeps a user's
contributions (papers, reviews) after they delete their account to preserve utility for others, but
could also associate each review with a different placeholder to avoid revealing the deleted
reviewer's identity.
%
%Likewise, contributions with a shared property (\eg Reddit posts with a common tag)
%might be removed entirely to avoid inference attacks, or retained and decorrelated from the
%property (\ie keeping the user's Reddit posts, but removing their tags).
%\eddie{Would like more explanation of that}
In some cases, a policy could preserve utility while reducing the efficacy of later inference
attacks by \eg modifying sensitive metadata (\eg tags, creation times).
%
%Some transformations should be performed only on sensitive metadata:
%Similar policies could apply \emph{only if} the property was created by the user (\eg keeping
%the user's Reddit posts, but removing any user-created tags),
% tag like “\#cat” is likely insensitive and useful to preserve, whereas one naming a person is not.
%
%(\eg remove the user's posts on Reddit with tag $t$ if these posts comprise more than 10\%
%of all posts with tag $t$).
%
Individual users could even specify different preferences for their data.
%
%% A privacy transformation framework is necessary to turn these preferences into concrete
%% operations without undue developer burden.
%

%
Applications could go beyond simple account deletion and support \textbf{data
expiration} policies, which anonymize a user's contributions after the user has been inactive for a
period of time, possibly restoring the user's profile and contributions if the user ever logs back
in.
%
Or the application could support gradual \textbf{data decay} policies that ``decay'' sensitive data
by applying incremental privacy transformations over time.

Furthermore, many applications might wish to employ \textbf{reversible} transformations to, for
example, support account reactivation instead of permanent and irrevocable account deletion.
%
After all, if services must allow users to remove their data on request, it is in the operator's
interest as well as the user's to make it easy for a user to change their mind and return, bringing
their data along.
%
%An advanced reversible transformation might record all data transformed, and push an encrypted
%archive to third-party cloud storage.
%
%If the user wishes to return, they supply the archive to reverse the transformation.
%
%To ensure access even if the user loses their key, the transformation might secret-share
%the encryption key~\cite{secretsharing} among the user, the service, and a trusted third party (\eg
%the ACLU or EFF).
%

%
%In particular, we imagine that developers declaratively specify the above and other
%transformation policies like they specify a storage structure (\eg a relational schema) today.
%
%This also allows for new policies and use cases that benefit both end-users and service operators.

\subsection{Challenges}
%
Although privacy transformations offer many potential benefits, practically supporting them is hard.
First, modifying or deleting data must not compromise application correctness: for example, privacy
transformations must maintain referential integrity of an application's relational database.
%
Furthermore, one application may apply several privacy transformations, intermixed with
transformation reversals.  When these transformations may share dependencies or contradict each
other, correctly composing transformations to achieve the desired privacy properties is a daunting
task.
%
Finally, practical privacy transformations should require minimal application changes.
%

%
To make these challenges concrete, suppose we were to implement the following two privacy
transformations in HotCRP. 
%
(1) \texttt{ConfAnon} implements universal conference anonymization, a form of data decay to
prevent reviewer identification in case of a data breach.
%
(2) \texttt{GDPR} implements GDPR's ``right-to-be-forgotten'' removal of user data.
%
\texttt{ConfAnon} decorrelates reviews and review metadata (\eg review preferences) from reviewers
by ensuring that no review references a real user's account (via a foreign key relationship).
To maintain referential integrity, decorrelation requires updating foreign key
attributes to reference fake user profiles.
\texttt{GDPR} finds and deletes the leaving user's data.
%

%
In isolation, each transformation may seem simple; however, composing them is tricky.
\texttt{ConfAnon} destroys information linking a user's data to their account by relinking this data
to anonymous, fake users (\eg rewriting the foreign key linking a review preference to a user to
point to a fake user profile).  If the user later invokes \texttt{GDPR}, the
disguise needs to know which data to remove, which is impossible without the original links.
%
Due to data dependencies between these transformations, \texttt{GDPR} fails to achieve its desired
privacy properties, namely the complete removal of the user's data.
%
