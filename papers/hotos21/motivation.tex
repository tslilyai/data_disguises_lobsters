\section{Privacy Transformations}
\label{sec:survey}

\subsection{Privacy Transformations In Practice Today}
%
We surveyed widely-used web applications to understand the privacy
transformations they apply.
%
The main privacy transformation these applications support is account deletion,
a transformation that \eg the GDPR~\cite[Art.\ 17]{eu:gdpr} mandates.
%
%~\cite{facebook:privacy, twitter:privacy, hotcrp:privacy, reddit:privacy,
%github:privacy, hackernews:privacy, strava:privacy, linkedin:privacy, stackoverflow:privacy,
%wikipedia:privacy, amazon:privacy, prestashop:privacy, spotify:privacy, lobsters:privacy}:


%\paragraph{Treatment of user contributions.}
%
On account removal, all applications surveyed delete some user profile
information (\eg passwords) from underlying databases, but all applications
also retain some information for legal or necessary business purposes
(\eg Spotify fraud detection~\cite{spotify:privacy}, PrestaShop/Amazon
orders~\cite{amazon:privacy, prestashop:privacy}).
%
The treatment of other user contributions varies.
%
Some services keep most contributions publicly and indefinitely available (\eg StackOverflow
answers~\cite{stackoverflow:privacy}, shared Strava routes~\cite{strava:privacy}), sometimes
associated with the original user name (\eg Wikipedia edit history~\cite{wikipedia:privacy}), even
if a user deletes their account.
%
Social networking platforms delete some of a user's contributions (\eg
posts), but keep others unanonymized and visible to their recipients (\eg
Facebook/Twitter private messages~\cite{facebook:privacy, twitter:privacy},
LinkedIn updates~\cite{linkedin:privacy}).
%
Other platforms with mostly public content keep user contributions visible to the intended
audience, but anonymize them by reattributing the contribution to a placeholder user
(\eg GitHub's ``@ghost''~\cite{github:privacy}, Reddit and Lobsters'
``[deleted]''~\cite{reddit:privacy, lobsters:privacy}).
%
%    \item Keep certain user contributions unanonymized and visible to its intended audience (\eg
%        HotCRP, Lobsters, Wikipedia, HackerNews~\cite{hotcrp:privacy, lobsters:privacy,
%        hackernews:privacy, wikipedia:privacy}).
%    \item Delete user contributions on user profile or feed (\eg Facebook,
%        Twitter~\cite{facebook:privacy, twitter:privacy}).
%\end{itemize}
%

In the open-source applications surveyed, developers implement privacy transformations
via ad-hoc database operations, and only trigger them on explicit, user-initiated account
deletion.
%
Most developers appear to pay little attention to identifying correlations
within the remaining data.
%


\subsection{Desirable Privacy Transformations Missing Today}

\paragraph{Nuanced policies.}
%
Users and application developers can both benefit from more nuanced privacy policies.
%
For example, a confidential paper review system like HotCRP~\cite{hotcrp} must keep a
user's contributions
(papers, reviews) after they delete their account to preserve utility for others, but could
associate each review with a different placeholder to avoid revealing the deleted reviewer's
identity.
%
%Likewise, contributions with a shared property (\eg Reddit posts with a common tag)
%might be removed entirely to avoid inference attacks, or retained and decorrelated from the
%property (\ie keeping the user's Reddit posts, but removing their tags).
%\eddie{Would like more explanation of that}
In some cases, a policy could preserve utility while reducing the efficacy of later inference
attacks by changing posts, such as by modifying their metadata (\eg tags, posting times).
%
Some transformations should be performed only on sensitive metadata:
%Similar policies could apply \emph{only if} the property was created by the user (\eg keeping
%the user's Reddit posts, but removing any user-created tags),
a tag like “\#cat” is likely insensitive and useful to preserve, whereas one naming a person is not.
%
%(\eg remove the user's posts on Reddit with tag $t$ if these posts comprise more than 10\%
%of all posts with tag $t$).
%
Individual users may even specify different preferences for their data.
%
%% A privacy transformation framework is necessary to turn these preferences into concrete
%% operations without undue developer burden.
%

\paragraph{Data decay.}
%
Applications could go beyond simple account deletion and support a data expiration policy that
anonymizes a user's contributions after the user has been inactive for a period of time,
possibly restoring the user's profile and contributions if the user ever logs back in.
%
Or the application could gradually ``decay'' sensitive data by applying several privacy
transformations that incrementally remove identifiable information over time.
%from it as it ages.
%
%% This requires periodic, automated privacy transformations.

\paragraph{Reversibility.}
%
Many applications might wish to employ \emph{reversible} transformations to, for example, support
account reactivation instead of permanent and irrevocable account deletion.
%
After all, if services must allow users to remove their data on request, it is in the operator's
interest as well as the user's to make it easy for a user to change their mind and return, bringing their data along.
%
%An advanced reversible transformation might record all data transformed, and push an encrypted
%archive to third-party cloud storage.
%
%If the user wishes to return, they supply the archive to reverse the transformation.
%
%To ensure access even if the user loses their key, the transformation might secret-share
%the encryption key~\cite{secretsharing} among the user, the service, and a trusted third party (\eg
%the ACLU or EFF).
%

\subsection{The Future of Privacy Transformations}
%
\ms{This doesn't fit well currently.}
\lyt{Took a shot a rewording/reorganizing...}
%
Privacy transformations and their potential properties allow them to act as powerful mechanisms for
privacy. This argues for a systematic treatment of privacy transformations that makes them a
first-class citizen in application design.
%
In particular, we imagine that developers declaratively specify the above and other
transformation policies like they specify a storage structure (\eg a relational schema) today.
%
This also allows for new policies and use cases that benefit both end-users and service operators.
%
%% Our \emph{data disguising} approach supports these new policies and concepts,
%% which are missing from today's applications but easily described via disguises.
%

\subsection{Challenges}
%
Supporting practical privacy transformations poses several challenges. First, modifying
or deleting data must not compromise application correctness: for example, privacy transformations
must maintain referential integrity of an application's relational database.
%
Furthermore, one application may apply several privacy transformations. Composed
transformations---multiple transformations applied in sequence---should achieve each individual
transformation's privacy properties, even when these transformations may share dependencies or
contradict each other.
%
Finally, practical privacy transformations should require minimal application changes.
%

%
To make these challenges concrete, suppose we were to implement the following two privacy
transformations in HotCRP. 
%
(1) \texttt{ConfAnon} implements universal conference anonymization, a form of data decay to
preventing past-conference reviewer identification in case of a data breach.
%
(2) \texttt{GDPR} implements GDPR's ``right-to-be-forgotten'' removal of user data.
%
\texttt{ConfAnon} decorrelates reviews and review metadata (\eg review preferences) from reviewers
by ensuring that no review references a real user's account (via a foreign key relationship).
To maintain referential integrity, decorrelation requires updating foreign key
attributes to reference fake user profiles.
\texttt{GDPR} finds and deletes the leaving user's data.
%

%
In isolation, each transformation may seem simple; however, composing them is tricky.
\texttt{ConfAnon} destroys information linking a user's data to their account by relinking this data
to anonymous, fake users (\eg the foreign key linking review preferences to a user has been rewritten to
point to a fake user profile).  If the user later invokes \texttt{GDPR}, the
disguise needs to know which data to remove, which is impossible without the original links.
%
Data dependencies between these transformations causes \texttt{GDPR} to
fail to achieve its desired privacy properties, namely the complete removal of the user's data.
%
