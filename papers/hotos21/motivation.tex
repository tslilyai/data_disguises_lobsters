\section{Privacy Transformations}
\label{sec:survey}

\subsection{Privacy Transformations In Practice Today}
%
We surveyed widely-used web applications to understand the privacy
transformations they apply.
%
The main privacy transformation these applications support is account deletion,
a transformation that \eg the GDPR~\cite[Art.\ 17]{eu:gdpr} mandates.
%
%~\cite{facebook:privacy, twitter:privacy, hotcrp:privacy, reddit:privacy,
%github:privacy, hackernews:privacy, strava:privacy, linkedin:privacy, stackoverflow:privacy,
%wikipedia:privacy, amazon:privacy, prestashop:privacy, spotify:privacy, lobsters:privacy}:


%\paragraph{Treatment of user contributions.}
%
On account removal, all applications surveyed delete some user profile
information (\eg passwords) from underlying databases, but all applications
also retain some information for legal or necessary business purposes
(\eg Spotify fraud detection~\cite{spotify:privacy}, PrestaShop/Amazon
orders~\cite{amazon:privacy, prestashop:privacy}).
%
The treatment of other user contributions varies.
%
Some services keep most contributions publicly and indefinitely available (\eg StackOverflow
answers~\cite{stackoverflow:privacy}, shared Strava routes~\cite{strava:privacy}), sometimes
associated with the original user name (\eg Wikipedia edit history~\cite{wikipedia:privacy}), even
if a user deletes their account.
%
Social networking platforms delete some of a user's contributions (\eg
posts), but keep others unanonymized and visible to their recipients (\eg
Facebook/Twitter private messages~\cite{facebook:privacy, twitter:privacy},
LinkedIn updates~\cite{linkedin:privacy}).
%
Other platforms with mostly public content keep user contributions visible to the intended
audience, but anonymize them by reattributing the contribution to a placeholder user
(\eg GitHub's ``@ghost''~\cite{github:privacy}, Reddit and Lobsters'
``[deleted]''~\cite{reddit:privacy, lobsters:privacy}).
%
%    \item Keep certain user contributions unanonymized and visible to its intended audience (\eg
%        HotCRP, Lobsters, Wikipedia, HackerNews~\cite{hotcrp:privacy, lobsters:privacy,
%        hackernews:privacy, wikipedia:privacy}).
%    \item Delete user contributions on user profile or feed (\eg Facebook,
%        Twitter~\cite{facebook:privacy, twitter:privacy}).
%\end{itemize}
%

In the open-source applications surveyed, developers implement privacy transformations
via ad-hoc database operations, and only trigger them on explicit, user-initiated account
deletion.
%
Most developers appear to pay little attention to identifying correlations
within the remaining data.
%


\subsection{Desirable Privacy Transformations Missing Today}

\paragraph{Nuanced policies.}
%
Users and application developers can both benefit from more nuanced privacy policies.
%
For example, a confidential paper review system like HotCRP~\cite{hotcrp} must keep a
user's contributions
(papers, reviews) after they delete their account to preserve utility for others, but could
associate each review with a different placeholder to avoid revealing the deleted reviewer's
identity.
%
%Likewise, contributions with a shared property (\eg Reddit posts with a common tag)
%might be removed entirely to avoid inference attacks, or retained and decorrelated from the
%property (\ie keeping the user's Reddit posts, but removing their tags).
%\eddie{Would like more explanation of that}
In some cases, a policy could preserve utility while reducing the efficacy of later inference
attacks by changing posts, such as by modifying their metadata (\eg tags, posting times).
%
Some transformations should be performed only on sensitive metadata:
%Similar policies could apply \emph{only if} the property was created by the user (\eg keeping
%the user's Reddit posts, but removing any user-created tags),
a tag like “\#cat” is likely insensitive and useful to preserve, whereas one naming a person is not.
%
%(\eg remove the user's posts on Reddit with tag $t$ if these posts comprise more than 10\%
%of all posts with tag $t$).
%
Individual users may even specify different preferences for their data.
%
%% A privacy transformation framework is necessary to turn these preferences into concrete
%% operations without undue developer burden.
%

\paragraph{Data decay.}
%
Applications could go beyond simple account deletion and support a data expiration policy that
anonymizes a user's contributions after the user has been inactive for a period of time,
possibly restoring the user's profile and contributions if the user ever logs back in.
%
Or the application could gradually ``decay'' sensitive data by applying several privacy
transformations that incrementally remove identifiable information over time.
%from it as it ages.
%
%% This requires periodic, automated privacy transformations.

\paragraph{Reversibility.}
%
Many applications might wish to employ \emph{reversible} transformations to, for example, support
account reactivation instead of permanent and irrevocable account deletion.
%
After all, if services must allow users to remove their data on request, it is in the operator's
interest as well as the user's to make it easy for a user to change their mind and return, bringing their data along.
%
%An advanced reversible transformation might record all data transformed, and push an encrypted
%archive to third-party cloud storage.
%
%If the user wishes to return, they supply the archive to reverse the transformation.
%
%To ensure access even if the user loses their key, the transformation might secret-share
%the encryption key~\cite{secretsharing} among the user, the service, and a trusted third party (\eg
%the ACLU or EFF).
%

\subsection{The Future of Privacy Transformations}
%
\ms{This doesn't fit well currently.}
\lyt{Took a shot a rewording/reorganizing...}
%
Privacy transformations and their potential properties allow them to act as powerful mechanisms for
privacy. This argues for a systematic treatment of privacy transformations that makes them a
first-class citizen in application design.
%
In particular, we imagine that developers declaratively specify the above and other
transformation policies like they specify a storage structure (\eg a relational schema) today.
%
This also allows for new policies and use cases that benefit both end-users and service operators.
%
%% Our \emph{data disguising} approach supports these new policies and concepts,
%% which are missing from today's applications but easily described via disguises.
%

\subsection{Challenges}
%
Properly supporting privacy transformations poses several challenges in practice. First, modifying or
deleting data must not compromise application correctness: for example, privacy transformations must
maintain referential integrity of an application's relational database.
%
Furthermore, one application may apply several privacy transformations. Composing
transformations---applying multiple transformations in sequence---should achieve a state satisfying
every transformations' privacy properties, even when these transformations may share dependencies or
contradict each other.
%
Finally, to make privacy transformations feasible in practice, application changes should be kept to
a minimum.
%

%
To make these challenges concrete, suppose we were to implement the following two privacy
transformations in HotCRP as SQL transactions. (1) \texttt{ConfAnon} implements universal conference
anonymization by reassociating reviews and other metadata (e.g., review preferences) with
placeholder users; this is a form of data decay, preventing leaking reviewer identities for historic data in
the case of a data breach. (2) \texttt{GDPR} implements GDPR's ``right-to-be-forgotten'' removal of user data.
%
\texttt{ConfAnon} decorrelates reviewer profiles from their contributions: no review should reference a
user's account (via a foreign key relationship).
Furthermore, decorrelation must maintain referential integrity, requiring changes to foreign key
attributes to reference some fake user profile.
\texttt{GDPR} must properly remove the leaving user's owned data.

While both of these transformations seem relatively straightforward to implement, composing even
these simple transformations is tricky. \texttt{ConfAnon} destroys links between a user's data and
their profile, making it impossible for a subsequent \texttt{GDPR} application to correctly remove
all of the user's data.
%
For instance, \texttt{ConfAnon} removes the link between a user and their bookmarked papers by
relinking the bookmarks to anonymous, fake users; if the user later invokes \texttt{GDPR}, the
disguise needs to know which bookmarks to remove, which is impossible without the
original links.
%
Data dependencies between the two transformations may thus cause the application of multiple
transformations to fail to achieve the desired privacy properties.
%
