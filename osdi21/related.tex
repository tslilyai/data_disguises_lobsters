%-------------------------------------------------------------------------------
\section{Background and Related Work}
%-------------------------------------------------------------------------------

\subsection{Anonymization in Web Applications}
Today's web applications are incentivized to retain as much user data on their platform as possible,
both to sell and to process for the application's targeting, and to increase the utility of their
platform for other users (e.g., keeping reviews or votes for other users of the application to use
to judge a product).

When users on web applications delete their accounts, some applications (e.g.,\ Lobst.rs and Reddit)
choose to de-identify user data upon account deletion by replacing unique user identifiers such as
usernames with global placeholders (``anonymous'') or a pseudonym (``anonymous\_mouse''). Other
applications (e.g.,\ Facebook and Twitter) delete (most of the) user's data.  The first choice
optimizes the amount of data the application retains, the second can lead to confusing semantics for
the application (e.g.,\ nonsensical comment threads). In both cases, accounts cannot be restored
after deletion. 

Applications such as Facebook or Twitter do support user account deactivation by hiding (most) of the
user's data, but retain all user data with their identifiers in their databases.
\lyt{Facebook keeps a ``log`` of data versions, but this wouldn't be a problem if everything in the
database is ghosted!}

\sys{}'s decorrelation provides stronger de-identification properties than a global placeholder, and
keeps as much data as possible in the application while still guaranteeing de-identification.
\sys{} also supports resubscription without allowing the application to retain identifying user data
in its database.

\subsection{Differentially Private Queries}

PINQ, a data analysis platform created by McSherry~\cite{pinq}, provides formal differential privacy
guarantees for any query allowed by the platform.  Data analysts using PINQ can perform
transformations (Where, Union, GroupBy, and restricted Join operations) on the underlying data
records prior to extracting information via aggregations (e.g., counts, sums, etc.) The aggregation
results include added noise to meet the given privacy budget $\epsilon$, ensuring that analysts only
ever receive $\epsilon$-differentially-private results.  PINQ calculates the privacy loss of any
given query based on transformations and aggregations to be performed; if a particular query exceeds
a predefined privacy budget, PINQ refuses to execute the query.

Differential privacy provides a formal framework that defines the privacy loss a user incurs
when the user's data is included in the dataset. However, the setting of differential privacy
that of \sys{} in several important ways. 

First, web applications' utility often derives from visibility into individual data records. PINQ
restricts JOIN transformation and exposes information only through differentially-private
aggregation mechanisms. While sufficient for many data analyses, this approach severely hinders web
application functionality. \sys{} supports all application queries, even ones that expose individual
records, such as \texttt{SELECT story.content, story.tag FROM stories LIMIT 10}.  
However, this makes formally defining the privacy guarantees of \sys{} more complex than
simply applying DP.  DP provides formal guarantees for results such as sums or averages because such
results are computed using well-defined mathematics, allowing us to neatly capture the total
knowledge gained by the adversary. However, in the world of web applications, the knowledge gained
via queries that may reveal individual data records cannot be so easily defined: adversaries may
learn information from the friends who liked the (ghosted) story, or the time and location the story
was posted.

Thus, we cannot, in general, use the DP approach of asking, "by how much do answers to adversary
queries change with the presence of a users' decorrelated data?" While we can precisely define which
data records an adversary sees with and without a user's decorrelated data, we cannot numerically
quantify the knowledge gained by the adversary in the same way as we can quantify the percentage
change to a sum or average.
Instead, \sys{} reduces identifying information in the changes induced by a users' (decorrelated)
data, rather than reducing the amount of change itself.

This is demonstrated by how PINQ and \sys{} differ in masking entities associated with unique keys.
PINQ restricts JOINs to group by unique keys. As an (imperfect) example, selecting a JOIN of stories
with users via the UID would result in a record, one per UID, which represents the ``group'' of all
stories associated with a user. A normal join would create a separate record per story, each
associated with the user of that UID.

Instead of lumping all matching stories together, \sys{} explodes the users' UID into individual
unique keys (ghosts), one per story. By PINQ's analysis of stable transformations, this leads to
unbounded stability (as the number of different results produced by queries is proportional to the
potentially very large number of stories for that user). In PINQ's DP framework, such unbounded
stability leads to unbounded privacy loss: one users' data could change aggregation results in
"significant" ways unless huge amounts of noise were added. 

While \sys{}'s approach has been shown to be problematic in the past (psuedo/anonymization
techniques still allow for reidentification/reconstruction attacks), \sys{} provides better
anonymization than prior approaches, which decorrelate only coarsely by associating remnants of data
with a global placeholder for all deleted users, or do not decorrelate at all (e.g., replacing
usernames with a pseudonym, as in the AOL dataset case~\cite{aol}). In addition, \sys{} and PINQ's
approaches can be complementary: for queries that are more analytic in nature (for example,
population statistics used by the web application to measure usage or trends), \sys{} can utilize
PINQ's technique to provide differentially private guarantees (for a particular snapshot of the
dataset).  \sys{}'s decorrelation, in fact, adds more noise than necessary in these cases: a count
of unique users who posted about cats will be more imprecise because each post by a decorrelated
user is now associated with a unique (ghost) user.

%\sys{} provides the highest granularity of decorrelation possible (for integer
%identifiers)\lyt{(Not sure about this claim for highest granularity, since noise / fake records
%could also be introduced?)}.
%
%Some examples of how \sys{} provides better privacy than prior approaches even with potentially unbounded DP-defined privacy
%loss:
%\begin{itemize}
%    \item Aspects like birthdate, sex, ZIP code are identifiable, even if usernames/UIDs are
%        deidentified --> all deanonymization records suffer this problem, if one data record holds
%        this information. Something like a story might only have one "bit" of information---needs to
%        be linked with other stories to be interesting at all.
%    \item 
%    \item Querying the count of people with HIV and 
%\end{itemize}
%\lyt{TODO: one user's data in mutliple queries cannot be ``linked''}
%
%\subsection{Deletion Frameworks}
%Companies have developed frameworks to avoid deletion bugs (e.g., DELF~\cite{delf} at Facebook), but many
%applications decorrelate only coarsely by associating remnants of data with a global placeholder for
%all deleted users, or do not decorrelate at all (e.g., replacing usernames with a pseudonym).
%
%\lyt{cite ones that replace all of one user with pseudoym---AOL anonymization failure: https://www.nytimes.com/2006/08/09/technology/09aol.html}
%
%\lyt{(cite Reddit, Lobsters, others?)}
%

%First, differential privacy guarantees are formally defined on a fixed dataset; \sys{} aims to
%ensure privacy in the context of changing datasets, as is common in social media settings where
%users constantly add, update, and delete content.  While differential privacy-defined loss can be
%computed for a particular query on data set snapshots and added together by treating snapshots as
%independent, potential privacy loss from queries performed on a flucuating dataset, where the
%flucuations in the dataset might themselves leak information, has not been formally defined.  PINQ supports
%data analysis queries only on a fixed dataset.


\subsection{Deletion Privacy}
The right to be forgotten has also been formally defined by Garg et
al.~\cite{garg}, where correct deletion corresponds to the notion of
leave-no-trace: the state of the data collection system after a user requests to be forgotten should
be left (nearly) indistinguishable from that where the user never existed to begin with. While
\sys{} uses a similar comparison, Garg et al.'s formalization assumes that users operate
independently, and that the centralized data collector prevents one user's data from influencing
another's.

Other related works:
\begin{itemize}
    \item Problems with anonymization + reidentification later on (see PINQ for references)
    \item Deceptive Deletions for protecting withdrawn posts: https://arxiv.org/abs/2005.14113
    \item "My Friend Wanted to Talk About It and I Didn't": Understanding Perceptions of
        Deletion Privacy in Social Platforms, user survey https://arxiv.org/pdf/2008.11317.pdf;
        talk about decoy deletion, prescheduled deletion strategies~\cite{myfw}
    \item Contextual Integrity
    \item ML Unlearning
    \item k-anonymization, pseudonymization
\end{itemize}
